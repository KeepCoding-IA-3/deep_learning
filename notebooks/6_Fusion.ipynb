{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Técnicas de fusión\n",
        "\n",
        "La idea fundamental consiste en aprovechar la información sobre diferentes fuentes, que es recogida por métodos distintos. Por ejemplo, podemos querer combinar texto e imágenes, texto y datos tabulares, series temporales y vídeo, etc.\n",
        "\n",
        "En definitiva, buscamos combinar en un único modelo de aprendizaje toda la información disponible, no siempre posible de procesar en un único stream, de cara a codificar en el modelo la relación entre las diferentes modalidades de entrada y las etiquetas.\n",
        "\n",
        "Contrario a lo que pudiera parecer, no existen tantas alternativas. Aquí veremos dos de las fundamentales vías para conseguir dicha fusión de modalidades. En la última sesión veremos una tercera vía."
      ],
      "metadata": {
        "id": "3vnmuiyobL_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares: modelos de modalidad\n",
        "\n",
        "Partimos del supuesto de que cada modalidad precisa de su propia arquitectura para ser procesada, o que quizás, hemos trabajado con anterioridad en ambas modalidades por separado, y por tanto contamos con modelos ya pre-entrenados en la tarea de interés."
      ],
      "metadata": {
        "id": "h2D7oDjvb9AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Input, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "# Si tuviéramos los modelos guardados en disco, podríamos hacer simplemente:\n",
        "# tabular_model = keras.models.load_model(\"tabular_model.h5\")\n",
        "# vision_model = keras.models.load_model(\"vision_model.h5\")\n",
        "\n",
        "\n",
        "# Definimos un modelo básico para los datos tabulares\n",
        "def create_tabular_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation=\"relu\", input_shape=input_shape),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(16, activation=\"relu\", name=\"tabular_features\"),  # Feature vector\n",
        "        Dense(3, activation=\"softmax\", name=\"tabular_output\")  # Final classification layer\n",
        "    ], name=\"TabularModel\")\n",
        "    return model\n",
        "\n",
        "# Hacemos lo propio para el modelo de vision\n",
        "def create_vision_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(64, (3,3), activation=\"relu\"),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(16, activation=\"relu\", name=\"vision_features\"),  # Feature vector\n",
        "        Dense(3, activation=\"softmax\", name=\"vision_output\")  # Final classification layer\n",
        "    ], name=\"VisionModel\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "V81iSAfJcQPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos inventaremos el problema, de tal forma que asumimos que las imágenes tiene tamaños `224 x 224 x 3`, mientras que en la tabla tenemos `10` características por fila:\n"
      ],
      "metadata": {
        "id": "xF-kv6EeckhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shapes\n",
        "tabular_input_shape = (10,)  # Example: 10 numerical features\n",
        "vision_input_shape = (224, 224, 3)  # Example: 224x224 RGB images"
      ],
      "metadata": {
        "id": "6dFwG2fNcuFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabular_model = create_tabular_model(tabular_input_shape)\n",
        "tabular_model.summary()"
      ],
      "metadata": {
        "id": "OXeNbSbMczD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vision_model = create_vision_model(vision_input_shape)\n",
        "vision_model.summary()"
      ],
      "metadata": {
        "id": "E2zdflvnc14I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Late-fusion\n",
        "\n",
        "En este tipo de fusión, las modalidades han aprendido ya a predecir la tarea final. Sin embargo, esperamos que donde una modalidad falle, la otra acierte, de tal manera que un pequeño clasificador sobre las moda"
      ],
      "metadata": {
        "id": "dnEYBmaBflTo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPraoa5UY40A"
      },
      "outputs": [],
      "source": [
        "def build_late_fusion_model():\n",
        "    # Consideramos los mismos inputs que cada modelo por separado\n",
        "    tabular_input = Input(shape=tabular_input_shape, name=\"tabular_input\")\n",
        "    vision_input = Input(shape=vision_input_shape, name=\"vision_input\")\n",
        "\n",
        "    # Obtenemos las predicciones finales para cada modalidad\n",
        "    tabular_pred = ## Code ##\n",
        "    vision_pred = ## Code ##\n",
        "\n",
        "    # Fusión de las probabilidades concatenadas\n",
        "    merged = ## Code ##\n",
        "    output = Dense(3, activation=\"softmax\", name=\"final_output\")(merged)\n",
        "\n",
        "    # Definición final del modelo con nuevo classificador al final\n",
        "    late_fusion_model = Model(inputs=[tabular_input, vision_input], outputs=output)\n",
        "    return late_fusion_model\n",
        "\n",
        "model = build_late_fusion_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early-fusion\n",
        "\n",
        "En este caso, en lugar de esperar a que los modelos parciales emitan una decisión final, construiremos un clasificador más sofisticado que no aprenda sobre la base de las probabilidades predichas, sino de la representación interna con la que los modelos de cada modalidad trabajaban.\n",
        "\n",
        "La idea que subyace es que la extracción de características puede estar bien afinada en ambas modalidades, pero que la parte final, la de decisión, puede en los modelos parciales no ser capaz de resolver la tarea de manera óptima debidas a las interrelaciones entre las modalidades. Es decir, si hay una fuerte relación entre los datos, en lugar de esperar a que cada modelo prediga, elegimos fusionar las representaciones para obtener un embedding mezcla de las modalidades.\n",
        "\n",
        "En el caso más básico, dicha combinación se implementa como una concatenación de embeddings."
      ],
      "metadata": {
        "id": "-HFzFG1_hDUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_early_fusion_model():\n",
        "    # Inputs\n",
        "    tabular_input = Input(shape=tabular_input_shape, name=\"tabular_input\")\n",
        "    vision_input = Input(shape=vision_input_shape, name=\"vision_input\")\n",
        "\n",
        "    tabular_intermediate = Model(\n",
        "      inputs=tabular_model.input, outputs=tabular_model.get_layer('tabular_features').output)\n",
        "    vision_intermediate = Model(\n",
        "      inputs=vision_model.input, outputs=vision_model.get_layer('vision_features').output)\n",
        "\n",
        "    tabular_features = tabular_intermediate(tabular_input)\n",
        "    vision_features = vision_intermediate(vision_input)\n",
        "\n",
        "    # Fusionamos la representación de ambas modalidades\n",
        "    merged = Concatenate()([tabular_features, vision_features])\n",
        "\n",
        "    # Añadimos un clasificador, que suele ser más complejo que en late-fusion\n",
        "    x = Dense(128, activation=\"relu\")(merged)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    output = Dense(3, activation=\"softmax\", name=\"final_output\")(x)\n",
        "\n",
        "    early_fusion_model = Model(inputs=[tabular_input, vision_input], outputs=output)\n",
        "    return early_fusion_model\n",
        "\n",
        "model = build_early_fusion_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mLCnZwp7huDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos observar, en ambos casos obtenemos modelos que nos permiten trabajar simultáneamente con ambas modalidades, optimizando la toma de decisiones en escenarios y problemas complejos.\n",
        "\n",
        "A continuación, os sugiero algunas cuestiones:\n",
        "\n",
        "- Pueden estos esquemas aplicarse en machine learning tradicional?\n",
        "- Es mejor late-fusion, o early-fusion?\n",
        "- Precisamos que los modelos parciales estén ya entrenados en ambas estrategias? Desarrollad la respuesta.\n",
        "- Podemos entrenar los modelos parciales de nuevos?\n",
        "- Qué datos (en cuanto a particiones de nuestro dataset) debemos emplear para entrenar los modelos resultantes de la fusión?\n",
        "- Es necesario que entrenemos un clasificador basados en capas `Dense` cuando utilizamos estas estrategias de fusión?\n"
      ],
      "metadata": {
        "id": "rnRZuOzTjZVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "\n",
        "def build_early_fusion_representation():\n",
        "    # Inputs\n",
        "    tabular_input = Input(shape=tabular_input_shape, name=\"tabular_input\")\n",
        "    vision_input = Input(shape=vision_input_shape, name=\"vision_input\")\n",
        "\n",
        "    tabular_intermediate = ## Code ##\n",
        "    vision_intermediate = ## Code ##\n",
        "\n",
        "    tabular_features = ## Code ##\n",
        "    vision_features = ## Code ##\n",
        "\n",
        "    # Fusionamos la representación de ambas modalidades\n",
        "    merged = Concatenate()([tabular_features, vision_features])\n",
        "\n",
        "    early_fusion_representation = Model(inputs=[tabular_input, vision_input], outputs=merged)\n",
        "    return early_fusion_representation\n",
        "\n",
        "model = build_early_fusion_representation()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jSSxkEEakbKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hubiéramos procesado nuestras muestras, tendríamos `N` muestras caracterizadas por embeddings de `32` elementos:"
      ],
      "metadata": {
        "id": "UkGyzlE3lEGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asignamos valores aleatorios a los embeddings que lograríamos\n",
        "X = tf.random.uniform((1000, 32))\n",
        "y = tf.cast(tf.random.uniform((1000,), minval=0, maxval=1) > 0.5, tf.int32)\n",
        "\n",
        "model = RidgeClassifier()\n",
        "_ = model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "kyGedhnHlOdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}